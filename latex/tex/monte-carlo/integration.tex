\section{Monte Carlo Integration}%
\label{sec:mcint}

Consider a function
\(f\colon \vb{x}\in\Omega\subset\mathbb{R}^n\mapsto\mathbb{R}\) and a
probability density on
\(\rho\colon \vb{x}\in\Omega\mapsto\mathbb{R}_{\geq 0}\) with
\(\rho(\Omega) > 0,\; \int_{\Omega}\rho(\vb{x})\dd{\vb{x}} = 1\).  By
multiplying \(f\) with a one in the fashion of \cref{eq:baseintegral},
the integral of \(f\) over \(\Omega\) can be interpreted as the
expected value \(\EX{F/\Rho}\) of the random variable \(F/\Rho\) under
the distribution \(\rho\). This is the key to most MC methods.
%
\begin{equation}
  \label{eq:baseintegral}
  I = \int_\Omega f(\vb{x}) \dd{\vb{x}} = \int_\Omega
  \qty[\frac{f(\vb{x})}{\rho(\vb{x})}] \rho(\vb{x}) \dd{\vb{x}} = \EX{\frac{F}{\Rho}}
\end{equation}
%
The expected value \(\EX{F/\Rho}\) can be approximated by calculating
the mean of \(F/\Rho\) with \(N\) finite samples
\(\{\vb{x}_i\}_{i\in\overline{1,N}}\sim\rho\) (distributed according
to \(\rho\)), where \(N\) is a very large integer.
%
\begin{equation}
  \label{eq:approxexp}
  \EX{\frac{F}{\Rho}} \approx
  \frac{1}{N}\sum_{i=1}^N\frac{f(\vb{x_i})}{\rho(\vb{x_i})}
  \xrightarrow{N\rightarrow\infty} I
\end{equation}
%
The convergence of \cref{eq:approxexp} is due to the nature of the
expected value \cref{eq:evalue-mean} and
variance \cref{eq:variance-mean} of the mean
\(\overline{X} = \frac{1}{N}\sum_i X_i\) of \(N\) uncorrelated random
variables \(\{X_i\}_{i\in\overline{1,N}}\) with the same distribution,
expected value \(\EX{X_i}=\mathbb{E}\) and variance
\(\sigma_i^2 = \sigma^2\).
%
\begin{gather}
  \EX{\overline{X}} = \frac{1}{N}\sum_i\EX{X_i} = \mathbb{E} \label{eq:evalue-mean}\\
  \sigma^2_{\overline{X}} = \sum_i\frac{\sigma_i^2}{N^2} =
                            \frac{\sigma^2}{N}  \label{eq:variance-mean}
\end{gather}
%
Because \(\frac{\sigma^2}{N}\xrightarrow{N\rightarrow\infty} 0\)
\cref{eq:approxexp} really converges to \(I\). For finite, but large
\(N\) the value of \(\EX{F/P}\) is distributed (in increasingly
good approximation) according to a normal distribution around \(I\)
with the variance \(\VAR{F/\Rho}\cdot N^{-1}\) as in \cref{eq:varI} by
virtue of the central limit theorem.
%
\begin{align}
  \VAR{\frac{F}{\Rho}} &= \int_\Omega \qty[I -
  \frac{f(\vb{x})}{\rho(\vb{x})}]^2 \rho({\vb{x}}) \dd{\vb{x}} =
  \int_\Omega \qty[\qty(\frac{f(\vb{x})}{\rho(\vb{x})})^2 -
  I^2]\rho({\vb{x}}) \dd{\vb{x}}   \label{eq:varI}
 \\
  &\approx \frac{1}{N - 1}\sum_i \qty[I -
  \frac{f(\vb{x_i})}{\rho(\vb{x_i})}]^2  \label{eq:varI-approx}
\end{align}
%
The goal now is to reduce \(\VAR{F/\Rho}\) to speed up the convergence
of \cref{eq:approxexp} and achieve higher accuracy with fewer function
evaluations. There are at least three angles of attack
in~\ref{eq:baseintegral}, namely the distribution \(\rho\), the
variable \(\vb{x}\), and the integration volume
\(\Omega\). Accordingly, some ways variance reductions can be
accomplished are choosing a suitable \(\rho\) (importance sampling),
by transforming the integral onto another variable or by subdividing
the integration volume into several sub-volumes of different size
while keeping the sample size constant in all sub-volumes (a special
case of stratified sampling).\footnote{There are of course still other
  methods like the multi-channel method.}\footnote{Of course,
  combinations of these methods can be applied as well.}  Combining
ideas from importance sampling and stratified sampling leads to the
\vegas\ algorithm~\cite{Lepage:19781an} that approximates the optimal
distribution of importance sampling by adaptive subdivision of the
integration volume into a grid.

The convergence of \cref{eq:approxexp} is independent of the
dimensionality of the integration volume as opposed to many other
numerical integration algorithms (trapezoid rule, Simpsons rule) that
usually converge like \(N^{-\frac{k}{n}}\) with
\(k\in\mathbb{N}_{>0}\) and \(n\) being the dimensionality.  MC
integration in its simplest form converges like \(N^{-\frac{1}{2}}\)
in \emph{all} dimensions. Because phase space integrals in particle
physics usually have a high dimensionality, MC integration is a
suitable approach for those problems. When implementing MC methods,
the random samples can be obtained through hardware or software random
number generators (RNGs). Most implementations utilize software RNGs
because they supply pseudo-random numbers in a reproducible way, which
facilitates deniability and comparability~\cite{buckley:2011ge}.

\subsection{Naive Monte Carlo Integration and Change of Variables}
\label{sec:naivechange}

The simplest choice for \(\rho\) is given
by \cref{eq:simplep}, the uniform distribution.
%
\begin{equation}
  \label{eq:simplep}
  \rho(\vb{x}) = \frac{1}{\int_{\Omega}1\dd{\vb{x'}}} =
  \frac{1}{\abs{\Omega}}
\end{equation}
%
With this distribution \cref{eq:approxexp} becomes
\cref{eq:approxexp-uniform}. In other words, \(I\) is just the mean
value of \(f\) in \(\Omega\), henceforth called \(\bar{f}\),
multiplied with the volume.
%
\begin{equation}
  \label{eq:approxexp-uniform}
  \EX{\frac{F}{\Rho}} \approx
  \frac{\abs{\Omega}}{N}\sum_{i=1}^N f(\vb{x_i}) = \abs{\Omega}\cdot\bar{f}
\end{equation}
%
The variance \(\VAR{I}=\VAR{F/\Rho}\) is now given
by \cref{eq:approxvar-I}. Note that the factor \(\abs{\Omega}\) gets
squared when approximating the integral by the sum.
%
\begin{equation}
  \label{eq:approxvar-I}
  \VAR{\frac{F}{P}} = \abs{\Omega}\int_\Omega f(\vb{x})^2 -
  \underbrace{\qty(\frac{I}{\abs{\Omega}})^2}_{=\bar{f}} \dd{\vb{x}} \equiv \abs{\Omega}\cdot\sigma_f^2 \approx
  \frac{\abs{\Omega}^2}{N-1}\sum_{i}\qty[f(\vb{x}_i) - \bar{f}]^2
\end{equation}
%
Applying this method to integrate the \(\qqgg\) cross section from
\cref{eq:crossec} over a \(\theta\) interval, equivalent to
\(\eta\in [-2.5, 2.5]\) with a target accuracy of
\(\sigma=\SI{1e-3}{\pico\barn}\) results in
\result{xs/python/xs_mc} with a sample size of
\result{xs/python/xs_mc_N}.

Changing variables and integrating \cref{eq:xs-eta} over \(\eta\) with
the same target accuracy yields~\result{xs/python/xs_mc_eta} with a
sample size of just~\result{xs/python/xs_mc_eta_N}. The dramatic
reduction in variance and sample size can be understood qualitatively
by studying \cref{fig:xs-int-comp}, which shows both integrands with
the same y-axis scaling and their standard deviation visualized. The
differential cross section in terms of
\(\eta\)~(\cref{fig:xs-int-eta}) is less steep than the differential
cross section in terms of \(\theta\)~(\cref{fig:xs-int-theta}) and
takes on large values over most of the integration interval. In
general, the Jacobian arising in variable transformation has the same
effect as the probability density in importance sampling. It can be
shown that importance sampling and change of variables are formally
equivalent (see \ref{sec:equap}).
%
\begin{figure}[ht]
  \centering
  \begin{subfigure}[c]{.49\textwidth}
    \plot{xs/xs_integrand}
    \caption[\(2\pi\dv{\sigma}{\theta}\) with integration
    boundaries]{\label{fig:xs-int-theta} The integrand arising from
      differential cross section \(\dv{\sigma}{\theta}\) with the
      integration borders visualized as gray lines.}
  \end{subfigure}
  \begin{subfigure}[c]{.49\textwidth}
    \plot{xs/xs_integrand_eta}
    \caption[Differential cross section for \(\qqgg\) with integration
    boundaries]{\label{fig:xs-int-eta} The differential cross section
      \(\dv{\sigma}{\eta}\) (see \cref{eq:xs-eta}) scaled by \(2\pi\)
      with the integration borders visualized as gray lines.}
  \end{subfigure}
  \caption{\label{fig:xs-int-comp} Comparison of two parametrisations
    of the differential cross section. The same y-axis scaling has
    been chosen to visualize the difference in variance.}
\end{figure}
%
\subsection{Integration with \vegas}
\label{sec:mcintvegas}

Subdividing the integration volume into sub-volumes and taking the same
number of samples in each volume (stratified sampling), gives optimal
results, when the variance in every sub-volume is the
same~\cite{Lepage:19781an}. In importance sampling, the optimal
probability distribution is given by \cref{eq:optimalrho}, where
\(f(\Omega) \geq 0\) is presumed without loss of generality. When
applying \vegas\ to multi dimensional integrals,~\cref{eq:optimalrho}
is usually modified to factorize into distributions for each variable
to simplify calculations.
%
\begin{equation}
  \label{eq:optimalrho}
  \rho(\vb{x}) = \frac{f(\vb{x})}{\int_\Omega f(\vb{y})\dd{y}}
\end{equation}
%
The idea behind \vegas\ is to subdivide \(\Omega\) into hypercubes
(create a grid), define \(\rho\) as step-function with constant value
on those hypercubes and iteratively approximating
\cref{eq:optimalrho}, instead of trying to minimize the variance
directly. As in stratified sampling, the sample number is kept the
same in each hypercube to realize \cref{eq:optimalrho}. In the end,
the samples are concentrated where \(f\) takes on the highest values
and changes most rapidly. This is done by subdividing the hypercubes
into smaller chunks, based on their contribution to the integral,
which is calculated through MC sampling, and then varying the
hypercube borders until all hypercubes contain the same number of
chunks.  So if the contribution of a hypercube is large, it will be
divided into more chunks than others. When the hypercube borders are
then shifted, this hypercube will shrink, while others will grow by
consuming the remaining chunks.  Repeating this step in so called
\vegas\ iterations will converge the contribution of each hypercube to
the same value. More details about the algorithm can be found
in~\cite{Lepage:19781an}.  Note that no knowledge about the integrand
is required. The probability density used by \vegas\ is given in
\cref{eq:vegasrho} with \(K\) being the number of hypercubes and
\(\Omega_i\) being the hypercubes themselves.
%
\begin{equation}
  \label{eq:vegasrho}
  \rho(\vb{x}) = K\cdot
  \begin{cases}
    \frac{1}{\abs{\Omega_i}} & \vb{x}\in\Omega_{i\in\overline{1,K}} \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

\begin{figure}[ht]
  \centering \plot{xs/xs_integrand_vegas}
  \caption[\(2\pi\dv{\sigma}{\theta}\) scaled to increments found by
  \vegas\ ]{\label{fig:xs-int-vegas} The same integrand as in
    \cref{fig:xs-int-theta} with \vegas-generated increments and
    weighting applied (\(f/\rho\)). The colored bands are the standard
    deviations of the distributions with matching color.}
\end{figure}
%
This algorithm has been implemented in \texttt{Python} and applied to
\cref{eq:crossec}.  In one dimension the hypercubes become simple
interval increments and applying \vegas\ to \cref{eq:crossec} with
\result{xs/python/xs_mc_θ_vegas_K} increments yields
\result{xs/python/xs_mc_θ_vegas} with
\result{xs/python/xs_mc_θ_vegas_N} function evaluations (including
\vegas\ iterations). This result is comparable with the one obtained
by parameter transformation in \cref{sec:naivechange}.  The sample
count \(N\) is the total number of evaluations of \(f\). The resulting
increments and the weighted integrand \(f/\rho\) are depicted in
\cref{fig:xs-int-vegas}, along with the original integrand and it is
intuitively clear, that the variance is being reduced. Smaller
increments correspond to higher sample density and lower weights,
flattening out the integrand.

Generally the result gets better with more increments, but at the cost
of more \vegas\ iterations. The intermediate values of those
iterations can be accumulated to improve the accuracy of the end
result~\cite[197]{Lepage:19781an}.

The \vegas\ algorithm can be adapted to \(n\) dimensions by using a
grid of hypercubes instead of intervals and using the algorithm along
each axis with a slightly altered weighting
mechanism~\cite[197]{Lepage:19781an}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../document"
%%% End:
