What the heck should be in there. Let's draft up an outline.

* Intro
** Importance of MC Methods :SHORT:
 - important tool in particle physics
   - not just numerical
 - somewhat romenatic: distilling information with entropy
 - validation of new theories
   - some predictions are often more subtle than just the existense of
     new particles
   - backgrounds have to be substracted
** Diphoton Process
 - feynman diags and reaction formula
 - higgs decay channel
 - dihiggs decay
 - pure QED
* Calculation of the XS :TOO_LONG:
** Approach
 - formalism well separated from underlying theory
   - but can fool intuition (spin arguments)
   - in the course of semester: learned more about the theory :)
 - translating feynman diagrams to abstract matrix elements straight
   forward
 - first try: casimir's trick
   - error in calculation + one identity unknown
 - second try: evaluating the matrices directly
   - discovered a lot of tricks
   - error prone
 - back to studying the formalism: completeness relation for real
   photons
   - a matter of algebraic gymnastics
   - boils down to some trace and dirac matrix gymnastics
   - mixing terms cancel out, not zero in themselves
 - resulting expression for ME essentially t/u channel propagator
   (1/(t*u)) and spin correlation 1 + cos(x)^2
 - only angular dependencies, no kinematics, "nice" factors
 - symmetric in θ
** Result + Sherpa
 - apply the golden rule for 2->2 processes
 - show plots and total xs
 - shape verified later -> we need sampling techniques first
* Monte Carlo Methods
 - one simple idea, can be exploited and refined
 - how to extract information from a totally unknown function
   - look at it -> random points are the most "symmetric" choice
   - statistics to the rescue
 - what does this have to do with minecraft
 - theory deals with truly random (uncorrelated) so that statistics
   apply, prng's cater to that: deterministic, efficient (we don't do
   crypto)

** Integration
 - integration as mean value
 - convergence due to law of large numbers
   - independent of dimension
   - trivially parallelism
 - result normal distributed with σ due to central limit theorem
 - goal: speeding up convergence
   1. modify distribution
   2. integration variable
   3. subdivide integration volume
 - all those methods can be somewhat intertwined
 - focus on some simple methods

*** Naive Integration
 - why mix in that distribution: we choose it uniform
 - integral is mean
 - variance is variance of function: stddev linear in Volume!
 - include result
 - rediculous sample size

**** TODO compare to other numeric

*** Change of Variables
 - drastic improvement by transf. to η
 - only works by chance (more or less)
   - pseudo rapidity eats up angular divergence
 - can be shown: same effect as propability density
 - implementation is different

*** VEGAS
 - a simple ρ: step function on hypercubes, can be trivially generated
 - effectively subdividing the integration volume
 - optimal: same variance in every cube
 - easier to optimize: approximate optimal rho by step function
 - clarify: use rectangular grid and blank out unwated edges with θ
   function
 - nice feature: integrand does not have to be smooth :)
 - similar efficiency as the travo case
   - but a lot of room for parameter adjustment and tuning

**** TODO research the drawbacks that led to VEGAS
**** TODO nice visualization of vegas working
**** TODO look at original vegas
   - in 70s/80s memory a constraint

** Sampling
 - why: generate events
   - same as exp. measurements
   - (includes statistical effects)
   - events can be "dressed" with more effects
 - usual case: we have access to uniformly distributed random values
 - task: convert this sample into a sample of another distribution
 - short: solve equation

*** Hit or Miss
 - we don't always know f, may have complicated (inexplicit) form
 - solve "by proxy": generate sample of g and accept with propability f/g
 - the closer g to f, the better the efficiency
 - simplest choice: flat upper bound
 - show results etc
 - one can optimize upper bound with VEGAS

*** Change of Variables
 - reduction of variance similar to integration
 - simplify or reduce variance
 - one removes the step of generating g-samples
 - show results etc
 - hard to automate, but intuition and 'general rules' may serve well
   - see later case with PDFs -> choose eta right away

*** Hit or Miss VEGAS
 - use scaled vegas distribution as g and to hit or miss
 - samples for g are trivial to generate
 - vegas again approximates optimal distribution
 - results etc
 - advantage: no function specific input
 - problem: isolated parts of the distribution can drag down
   efficiency
   - where the hypercube approx does not work well
   - especially at discontinuities

**** TODO add pic that i've sent Frank

*** Stratified Sampling
 - avoid global effects: subdivide integration interval and sample
   independently
 - first generate coarse samples and distribute them in the respective grid points
 - optimizing: make cubes with low efficiency small! -> VEGAS
 - this approach was used for the self-made event generator and
   improved the efficiency greatly (< 1% to 30%)
 - disadvantage: accuracies of upper bounds and grid weights has to be
   good
   - will come back to this

*** Observables
 - particle identities and kinematics determine final state
 - other observables can be calculated on a per-event base
   - as can be shown, this results in the correct distributions
     without knowledge of the Jacobian

** Outlook
 - of course more methods
 - Sherpa exploits form propagators etc
 - multichannel uses multiple distributions for importance sampling
   and can be optimized "live"
   - https://www.sciencedirect.com/science/article/pii/0010465594900434
*** TODO Other modern Stuff

* Toy Event Generator
** Basics :SHORT:
** Implementation
** Results

* Pheno Stuff
** Shortcomings of the Toy Generator
** Short review of HO Effects
** Presentation and Discussion of selected Histograms

* Wrap-Up
** Summary
** Lessons Learned (if any)
** Outlook
